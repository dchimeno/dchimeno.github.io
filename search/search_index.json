{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>My name is Daniel Chimeno. I\u2019m a software engineer, and am at my happiest when I\u2019m building something cool.</p> <p>Here you will find a mix of notes, snippets, a bit longer posts, and links to various things I\u2019ve done.</p>"},{"location":"about/","title":"About me","text":"<p>TBD :)</p>"},{"location":"now/","title":"Now","text":"<p>This is my now page \u2014a public snapshot of my current top priorities, inspired by the nownownow.com initiative.</p> <ul> <li>Advocating for the benefits of post-aggregation, particularly with ClickHouse.  </li> <li>Transitioning projects from pre-aggregation to post-aggregation.  </li> <li>Expanding my knowledge of ClickHouse, data formats, and data ordering.</li> <li>Devoting time to my family.</li> <li>Improving this blog, sharing content and stepping into IndieWeb.</li> </ul>"},{"location":"blog/","title":"blog","text":""},{"location":"blog/choosing-a-stack-for-a-dev-blog/","title":"Choosing a stack for a dev blog nowadays","text":"<p>This year's resolution was to try to get more public - internet - exposure. The main reasons:</p> <ul> <li>Connect with people outside who share my interests.</li> <li>Consume less, create more: For years, I've been learning a lot from the internet. Now it's time to share.</li> <li>Trust your own \u201cCuriosity Barometer.\u201d If something sparks your interest in a unique way, it\u2019s likely to resonate with others too. Write about it, share it, and see what unfolds.</li> </ul> <p>Now, to align with the title: What should I use to publish it? When there are so many options available, I tend to simplify the decision to avoid getting trapped in analysis paralysis, so:</p>"},{"location":"blog/choosing-a-stack-for-a-dev-blog/#requirements","title":"Requirements","text":"<ol> <li>Being able to write in Markdown, as I'm comfortable with the syntax.</li> <li>Should include blog-like content and flat pages.</li> <li>Being able to create a kind of knowledge base or second brain.</li> <li>Being able to customize it.</li> <li>I should ship it in a morning or two.</li> </ol>"},{"location":"blog/choosing-a-stack-for-a-dev-blog/#first-decision-server-side-or-static-site","title":"First Decision: Server-Side or Static Site?","text":"<p>This was easy with requirement #5. Although I'm a backend developer, I know if I go down that route, I won't ship it fast as I have so many ideas to build. So, static it is.</p> <p>After searching and prompting for a while, three options seemed good:</p>"},{"location":"blog/choosing-a-stack-for-a-dev-blog/#mkdocs-material","title":"MkDocs (Material)","text":"<ol> <li>Pure Markdown, and I already built some project documentation with it.</li> <li>Ok.</li> <li>No specific way to create a KB.</li> <li>I should be able to. Just Python and some JS.</li> <li>Probably yes.</li> </ol>"},{"location":"blog/choosing-a-stack-for-a-dev-blog/#docusaurus","title":"Docusaurus","text":"<ol> <li>Markdown and some MDX (React-based).</li> <li>Ok.</li> <li>No specific way to create a KB.</li> <li>I should be able to, but I prefer Python.</li> <li>Likely.</li> </ol>"},{"location":"blog/choosing-a-stack-for-a-dev-blog/#quartz","title":"Quartz","text":"<ol> <li>Markdown.</li> <li>Ok, although it doesn't seem easy to customize the layout.</li> <li>Very specific to KB\u2014great!</li> <li>JS, components\u2026</li> <li>Not likely.</li> </ol>"},{"location":"blog/choosing-a-stack-for-a-dev-blog/#decision","title":"Decision","text":"<p>So, I went with MkDocs and the Material theme, prioritizing content over complexity. Let's ship!</p>"},{"location":"blog/please-order-your-data/","title":"Please, Order Your Data","text":"<p>Over the last five years, I've worked on various projects dealing with 'large' datasets in the analytics space. While I've fully embraced the benefits of ordering data, I've realized that many projects overlook this crucial aspect.</p> <p>In this post, I'll summarize the benefits of ordering data in case I need to revisit them or if they prove helpful to others.</p> <p>Let's start with a fictional dataset of cell tower events with values over time.</p> <p>Warning</p> <p>The examples here use DuckDB for convenience, but the concepts apply to the dataset itself, not the tool.</p> <p>We are working with a relatively small data sample, but it should be enough to demonstrate the benefits.</p>"},{"location":"blog/please-order-your-data/#first-step-know-your-data","title":"First Step: Know Your Data","text":"<pre><code>describe select * from 's3://bucket/tower/data_unordered.parquet'\n</code></pre> <pre><code>column_name         |column_type|null|key|default|extra|\n--------------------+-----------+----+---+-------+-----+\neventdate           |TIMESTAMP  |YES |   |       |     |\nECGI                |VARCHAR    |YES |   |       |     |\nLOCATION            |VARCHAR    |YES |   |       |     |\nVENDOR              |VARCHAR    |YES |   |       |     |\nVALUE_BIN_1         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_2         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_3         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_4         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_5         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_6         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_7         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_8         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_9         |DOUBLE     |YES |   |       |     |\nVALUE_BIN_10        |DOUBLE     |YES |   |       |     |\n...\nVALUE_BIN_30        |DOUBLE     |YES |   |       |     |\n</code></pre> <pre><code>select count(*) from 's3://bucket/tower/data.parquet'\n-- 7,781,925\nSELECT sum(total_compressed_size)/(1024*1024) as MiB FROM parquet_metadata('s3://bucket/tower/data_unordered.parquet');\n-- 573 MiB\n</code></pre> <ul> <li>eventdate: When the event occurred.</li> <li>ECGI: A global and public identifier for cell towers.</li> <li>LOCATION and VENDOR: Attributes of the tower, which should remain the same for the same ECGI.</li> <li>VALUE_BIN_X: These are the values we measure at different distances.</li> </ul> <p>The source data has no inherent order.</p> <p>A typical query might look like this:</p> <pre><code>select sum(VALUE_BIN_2) from 's3://bucket/tower/data_unordered.parquet'\nwhere eventdate = '2024-02-02' and ecgi in ('XXX', 'YYYY')\n-- done in 5.1s\n</code></pre> <p>The table has a wide format, meaning multiple columns represent values. This structure has some downsides:   - We need to specify the column name in the SELECT statement.   - Joins with this table using selected bins become more complex.</p> <p>So, we transform and order the data as follows:</p> <pre><code>COPY (\n  WITH unpivoted AS (\n    SELECT\n      eventdate,\n      ECGI,\n      LOCATION,\n      VENDOR,\n      UNNEST(RANGE(1, 31))::UTINYINT as BIN,\n      UNNEST(ARRAY[\n        VALUE_BIN_1, VALUE_BIN_2, VALUE_BIN_3, VALUE_BIN_4, VALUE_BIN_5,\n        VALUE_BIN_6, VALUE_BIN_7, VALUE_BIN_8, VALUE_BIN_9, VALUE_BIN_10\n      ])::DOUBLE as BIN_VALUE\n    FROM 's3://bucket/tower/data_unordered.parquet'\n  )\n  SELECT * FROM unpivoted\n  ORDER BY eventdate, ECGI, TIMING_ADV_SET_INDEX, BIN\n)\nTO 's3://bucket/tower/cells_unpivot_ordered.parquet' (FORMAT PARQUET);\n</code></pre> <p>Note: We could use the <code>UNPIVOT</code> function instead of <code>UNNEST</code>.</p> <p>Now, let's check the transformed data:</p> <pre><code>select count(*) from 's3://bucket/tower/cells_unpivot_ordered.parquet'\n-- 233,457,750\nSELECT sum(total_compressed_size)/(1024*1024) as MiB FROM parquet_metadata('s3://bucket/tower/data_unordered.parquet');\n-- 591 MiB\n</code></pre> <p>Querying the ordered and transformed dataset:</p> <pre><code>select sum(BIN_VALUE) from 's3://bucket/tower/cells_unpivot_ordered.parquet'\nwhere eventdate = '2024-02-02 00:00:00' and ecgi in ('XXX', 'YYYY') and bin = 2\n-- done in 2s\n</code></pre> <p>With the same storage size, we now store 30x more rows (since there are 30 <code>VALUE_BIN_X</code> in the real dataset), and queries run 2.5x faster than before.</p>"},{"location":"blog/please-order-your-data/#bonus-compression-optimization","title":"Bonus: Compression Optimization","text":"<p>DuckDB's default compression library is Snappy, but we can configure it to use Zstandard (zstd), which generally provides better compression ratios while maintaining good decompression speed.</p> <pre><code>COPY\n  (SELECT * FROM 's3://bucket/tower/cells_unpivot_ordered.parquet')\n  TO 's3://bucket/tower/cells_unpivot.zstd.parquet'\n  (FORMAT 'parquet', COMPRESSION 'zstd');\n</code></pre> <pre><code>SELECT SUM(BIN_VALUE) FROM 's3://bucket/tower/cells_unpivot.zstd.parquet'\nWHERE eventdate = '2024-04-12 00:00:00' AND ecgi IN ('214050460033012','214050460033022') AND bin=2;\n-- 2.1s\n\nSELECT sum(total_compressed_size)/(1024*1024) as MiB FROM parquet_metadata('s3://bucket/tower/cells_unpivot.zstd.parquet');\n-- 248 MiB\n</code></pre>"},{"location":"blog/please-order-your-data/#results-summary","title":"Results Summary","text":"State Rows Size Query Time (s) Original 7,781,925 573 MiB 5.1 s After 233,457,750 248 MiB 2.1 s"},{"location":"blog/please-order-your-data/#conclusion","title":"Conclusion","text":"<p>Ordering data significantly improves performance by optimizing storage layout and query execution. In this case, we saw 30x more rows stored while reducing query time and size by more than half.</p> <p>However, this is only part of the optimization story. Other important techniques include:</p> <ul> <li>Optimizing data types: Using more compact data types reduces memory usage and speeds up queries.</li> <li>Custom codecs: Choosing the right codec before compression can further improve storage efficiency. In this case, DuckDB is doing it automatically</li> </ul> <p>These additional steps can further enhance data efficiency, reducing costs and improving performance even more.</p>"},{"location":"kb/clickhouse/","title":"","text":""},{"location":"kb/clickhouse/#see-data-sizes","title":"See data sizes","text":"<pre><code>SELECT\n    database,\n    table,\n    count() AS parts,\n    uniqExact(partition_id) AS partition_cnt,\n    sum(rows),\n    formatReadableSize(sum(data_compressed_bytes) AS comp_bytes) AS comp,\n    formatReadableSize(sum(data_uncompressed_bytes) AS uncomp_bytes) AS uncomp,\n    uncomp_bytes / comp_bytes AS ratio\nFROM system.parts\nWHERE active AND table like '%name%'\nGROUP BY\n    table\nORDER BY comp_bytes DESC\n</code></pre>"},{"location":"kb/clickhouse/#show-table-definitions","title":"Show table definitions","text":"<pre><code>select name, engine, data_paths, engine_full, partition_key, sorting_key, primary_key\nfrom  system.tables WHERE database !='system' FORMAT Vertical;\n</code></pre>"},{"location":"kb/clickhouse/#view-size-of-columns","title":"view size of columns","text":"<pre><code>SELECT\n    table,\n    name,\n    type,\n    compression_codec,\n    formatReadableSize(data_compressed_bytes) as comp,\n    formatReadableSize(data_uncompressed_bytes) as unc\nFROM system.columns\nWHERE table LIKE '%raw%'\nORDER BY table ASC, data_compressed_bytes desc\n</code></pre>"},{"location":"kb/clickhouse/#view-data-distribution","title":"view data distribution","text":"<pre><code>SELECT\n   count(),\n   * APPLY (uniq),\n   * APPLY (max),\n   * APPLY (min),\n   * APPLY(topK(5))\nFROM tablename\nFORMAT Vertical\n</code></pre>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/blog/","title":"blog","text":""}]}